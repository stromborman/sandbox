Executive Summary:

I used LogisticRegression (LR) and GradientBoostingClassifier (GB) from the scikit-learn package for python, which I will now briefly summarize. 

LR seeks to fit the data by using the logistic function f(x) = 1/(1+e^-x).  If there are d features, one seeks a vector of weights w in R^d and c in R so that p_i = f(w*X_i + c) is the predicted probability of observation i being true where X_i is the vector of features for observation i.  The optimization problem is then to minimize the sum of the l2-norm of w and a scalar multiple of the logloss of the predictions.  Strengths of LR is that it is relatively simple, converges quickly, and is highly effective when the explanatory features are linearly related with the target.  In turn, its weakness is that it is not effective when there are strong non-linear effects on the target. 

GB is built as the sum of simple decision tree classifiers F_n(X) = sum_j^n h_j(X).  At each stage a new simple decision tree h_new(X) is added in a way that amounts to doing a negative gradient flow of the loss function loss(y,F_n) in the space of functions where F_n lives.  The strength of GB comes from its ability to adapt its complexity as needed (by increasing the number of stages) and does not have trouble with non-linear effects.  (I now realize that I used linear correlation for feature selection, which may have weakened the GB model I built.)  There are also extensions of GB that can handle categorical data directly and missing data without needing to impute (Histogram-based Gradient Boosting Classification), which I tried but ran into a bug.  Weaknesses of GB is that it it can take much longer to converge (this prevented me from being able to optimize my model).

For my models, using cross-validation on the training data gave me a mean AUC of 0.7626 for LR and 0.7546 for GB.  Since these are rather close, I predict that my LR model will perform better since I am more confident that it was not over-fitted.  If I had spent more time and computer power on the GB model, I probably could have made it stronger than the LR model.

As for convincing a business partner that one model was better than the other, it would depend a lot on the context (which I do not have in this case).  An important first question would how do the costs of a false positives and false negatives compare?  If we are trying to detect insurance fraud, then missing a case where fraud happened is a lot more costly than the salary-time it would take for a human to determine the model was wrong and there was no fraud.  Conversely if this was about detecting new customers, a false positive means losing the per unit cost of advertising/communication while a false negative is a lost of profit the customer represents.  Assuming the models were as easy to maintain, with the known context one could set up an easy analysis to see which model would bring in more money.
